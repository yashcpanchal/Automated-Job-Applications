import os
from typing import List, TypedDict, Annotated, Optional
import asyncio
import httpx
from bs4 import BeautifulSoup
import json

from pydantic import BaseModel, Field

# Langchain and Langgraph imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langchain_community.tools import BraveSearch

# Models imports
from models.job import Job

# API key imports
from core.config import GOOGLE_API_KEY, BRAVE_SEARCH_API_KEY

class AgentState(TypedDict):
    """
    Represents the state of our job search agent.

    Attributes:
        resume_text: The user's resume content.
        search_prompt: The user's specific search request (e.g., "remote backend jobs").
        search_queries: A list of optimized search queries generated by the LLM.
        retrieved_urls: A list of URLs found by the search tool.
        extracted_jobs: A list of Job objects extracted from the URLs.
        final_jobs: The final, filtered list of jobs to be presented to the user.
    """
    resume_text: str
    search_prompt: str
    search_queries: List[str]
    retrieved_urls: List[str]
    extracted_jobs: List[Job]
    final_jobs: List[Job]

class SearchQueries(BaseModel):
    """List of search queries to use."""
    queries: List[str] = Field(
        ...,
        description="""A list of 3-5 diverse, expert-level search engine queries to find job postings."""
    )

async def _fetch_and_extract_job_data(url: str, llm_chain, semaphore: asyncio.Semaphore) -> Optional[Job]:
    """
    Fetches content from a URL, cleans it, and uses an LLM to actually extract the job data.
    Fetching and cleaning is done through bs4, the rest is done thru the llm chain. LLM chain will append
    all of the relevant data into the job model when it is invoked with the page_text passed in.
    """
    # Will only enter the function if the semaphore allwos it
    async with semaphore:
        print(f"Analyzing URL: {url}")
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.get(url, follow_redirects=True)
                response.raise_for_status() # Will raise an http error for bad status codes
            soup = BeautifulSoup(response.text, "html.parser")
            page_text = soup.get_text(separator=' ', strip=True)[:60000]
            if not page_text:
                print("Skipping {url}: No text content found")
                return None
            
            # Feeds into llm chain
            extracted_data = await llm_chain.ainvoke({"page_text": page_text})

            if extracted_data:
                extracted_data.source_url = url
                print(f"Extracted {extracted_data.title} from URL")
                return extracted_data
        
        except httpx.RequestError as e:
            print(f"Skipping {url}: Network error - {e}")
        except Exception as e:
            print(f"Skipping {url}: Error during processing - {e}")
        return None

async def craft_query_node(state: AgentState):
    """
    Node 1: Crafts targeted search queries based on the user's resume and prompt.
    """
    resume = state['resume_text']
    prompt = state['search_prompt']

    # Generate the search query here

    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=GOOGLE_API_KEY, temperature=0.2)
    # Ensuring that the llm sticks to the desired output schema
    structured_llm = llm.with_structured_output(SearchQueries)

    # Define the prompt template
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a world-class career assistant and expert at crafting search engine queries.
        Your goal is to help a user find relevant job postings based on their resume and a specific prompt.
        
        Generate a diverse list of 3 to 5 search queries that are likely to yield direct job application links
        on company career pages. The queries should be
        creative and varied to cover different angles of the job search."""),
        ("user", """Here is my resume:
        <resume>
        {resume_text}
        </resume>
        \n\n
        Here is my current search prompt:
        <prompt>
        {search_prompt}
        </prompt>
        \n\n
        Please generate the search queries.""")
    ])

    chain = prompt | structured_llm

    result_model = await chain.ainvoke({
        "resume_text": state['resume_text'],
        "search_prompt": state['search_prompt']
    })

    # queries is defined by the output schema in the SearchQueries class
    print(f"GENERATED QUERIES: {result_model.queries}")
    # breakpoint()
    return {"search_queries": result_model.queries}

async def retrieve_and_parse_node(state: AgentState):
    """
    Node 2: Uses a search tool to find job URLs and then parses each URL to extract job data.
    """
    queries = state['search_queries']
    search_tool = BraveSearch(api_key=BRAVE_SEARCH_API_KEY, return_direct=True, search_kwargs={"count": 20, "offset": 0}) # other params: freshness, result_filter 
    all_urls = set() # ensures no duplicates

    # Each entry in search_tasks is a call to brave search api with query
    # ainvoke creates an awaitable object which needs to be ran with asyncio
    for query in queries:
        results = await search_tool.ainvoke(query)
        results_list = json.loads(results)
        try:
            for result in results_list:
                if isinstance(result, dict) and 'link' in result:
                    all_urls.add(result['link'])
                else:
                    print("ALERT THERE IS AN ERROR HERE RESULT IS NOT A DICT")
                    breakpoint()
        except json.JSONDecodeError:
            print(f"Could not decode JSON from search results for query: {query}")
        
        await asyncio.sleep(1) # Wait 1 sec bc of Brave's 1 req/sec limit

    found_urls = list(all_urls)
    print(f"INFO: Found {len(found_urls)} unique URLs.")
    print(found_urls)

    # Another agent to parse through the search results retrieved by the search api and format them properly
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", google_api_key=GOOGLE_API_KEY)
    structured_llm = llm.with_structured_output(Job, include_raw=False)

    extraction_prompt = ChatPromptTemplate.from_messages([
        ("system", f"""You are an expert data extraction agent. Your task is to extract job posting information
        from the provided text content of a web page.
        
        You must extract the information into the following JSON schema:
        {Job.model_json_schema()}
        
        Pay close attention to finding the direct application URL. If a field is not present, use null.
        Do not invent any information."""),
        ("user", "Page Text:\n\n{page_text}")
    ])
    # Creating the llm chain using lcel
    extraction_chain = extraction_prompt | structured_llm
    # Create the semaphore
    semaphore = asyncio.Semaphore(5)
    # Creating a list of awaitable objects (bc function is async).
    extraction_tasks = [_fetch_and_extract_job_data(url, extraction_chain, semaphore) for url in found_urls]
    # Runs the awaitable objects in extraction_tasks using asyncio
    extracted_job_results = await asyncio.gather(*extraction_tasks)
    # Filters out emptyy jobs in extracted_job_results
    extracted_jobs = [job for job in extracted_job_results if job is not None]

    print(f"Successfully retrieved {len(extracted_jobs)} jobs.")

    return {"retrieved_urls": found_urls, "extracted_jobs": extracted_jobs}

async def process_and_match_node(state: AgentState):
    """
    Node 3: Filters the extracted jobs for relevance against the user's resume.
    """
    print("--- NODE: PROCESSING AND MATCHING JOBS ---")
    
    # TODO: Implement the final filtering logic.
    # This is where we could use semantic search (vector similarity) to compare
    # each job description to the user's resume text.
    print("INFO: Filtering jobs for relevance (mock implementation)...")
    
    # For now, we'll just assume all extracted jobs are relevant.
    return {"final_jobs": state['extracted_jobs']}


class JobSearchService:
    def __init__(self):
        workflow = StateGraph(AgentState)

        # Add the nodes to the graph
        workflow.add_node("craft_query", craft_query_node)
        workflow.add_node("retrieve_and_parse", retrieve_and_parse_node)
        workflow.add_node("process_and_match", process_and_match_node)

        # Set the entry and finish points of the graph
        workflow.set_entry_point("craft_query")
        workflow.add_edge("craft_query", "retrieve_and_parse")
        workflow.add_edge("retrieve_and_parse", "process_and_match")
        workflow.add_edge("process_and_match", END)

        # Compile the graph into a runnable application
        self.app = workflow.compile()

    async def search_and_process_jobs(self, resume_text: str, search_prompt: str) -> List[Job]:
        """
        The main method to run the job search agent.
        """
        print(" --- STARTING AGENTIC JOB SEARCH --- ")
        inputs = {"resume_text": resume_text, "search_prompt": search_prompt}
        final_state = await self.app.ainvoke(inputs)
        print(" --- AGENTIC JOB SEARCH COMPLETE --- \n")
        final_jobs = final_state.get('final_jobs', [])
        if final_jobs:
            final_jobs_as_dicts = [job.model_dump() for job in final_jobs]
            print(F"FINAL RESULTS:\n {json.dumps(final_jobs_as_dicts, indent=2)}")
        else:
            print("No jobs were found or extracted successfully unfortunately :(")

        return final_state.get('final_jobs', [])
